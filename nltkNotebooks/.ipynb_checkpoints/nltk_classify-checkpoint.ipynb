{
 "metadata": {
  "name": "",
  "signature": "sha256:83811a09cf5d32d4ab1655ada892f50b30b54733261612af6ac6ac29d3fb54d9"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import csv\n",
      "\n",
      "FIELDNAMES = ('polarity', 'id', 'date', 'query', 'author', 'text')\n",
      "\n",
      "# training_Data.csv file contains 1,600,000 tweets from sentiment140\n",
      "\n",
      "# for computation purpose we collect 200,000 positive and negative tweets only\n",
      "\n",
      "positive_training_tweets = []\n",
      "\n",
      "negative_training_tweets = []\n",
      "\n",
      "neutral_training_tweets = []\n",
      "\n",
      "total_tweets_added = 0\n",
      "\n",
      "with open('data/training_data.csv','rb') as file:\n",
      "    reader = csv.DictReader(file, fieldnames = FIELDNAMES, delimiter=',', quotechar='\"')\n",
      "    for row in reader:\n",
      "        if (int(row['polarity']) == 0) and (len(negative_training_tweets) < 200000):\n",
      "            negative_training_tweets.append(row['text'])\n",
      "            total_tweets_added += 1\n",
      "        elif (int(row['polarity']) == 2) and (len(neutral_training_tweets) < 200000):\n",
      "            neutral_training_tweets.append(row['text'])\n",
      "            total_tweets_added += 1\n",
      "        elif (int(row['polarity']) == 4) and (len(positive_training_tweets) < 200000):\n",
      "            positive_training_tweets.append(row['text'])\n",
      "            total_tweets_added += 1\n",
      "            \n",
      "        if total_tweets_added < 600000:\n",
      "            continue\n",
      "        else:\n",
      "            break\n",
      "\n",
      "negative_training_tweets[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 1,
       "text": [
        "\"@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\""
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "import string\n",
      "\n",
      "\n",
      "\"\"\"\n",
      "In this function we preprocess tweet by doing:\n",
      "\n",
      "1. lowercase the tweet\n",
      "2. remove urls\n",
      "3. remove @mentions\n",
      "4. remove punctuation, whitespaces, etc.\n",
      "5. replace #hash with hash\n",
      "\n",
      "\"\"\"\n",
      "def preProcessTweet(tweet):\n",
      "    \n",
      "    #lowercase the tweet\n",
      "    tweet = tweet.lower()\n",
      "    \n",
      "    #remove urls by matching www. http:\n",
      "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','',tweet)\n",
      "    \n",
      "    #remove @[] mentions\n",
      "    tweet = re.sub('@[^\\s]+','',tweet)\n",
      "    \n",
      "    #Replace #word with word\n",
      "    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
      "    \n",
      "    # remove leading, trailing whitespaces; newlines; tabs; duplicate whitespaces\n",
      "    tweet = \" \".join(tweet.split())\n",
      "    \n",
      "    # remove punctuation using sting translate\n",
      "    tweet = tweet.translate(string.maketrans(\"\",\"\"), string.punctuation)\n",
      "    \n",
      "    return tweet"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 56
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import stopwords\n",
      "from nltk.tokenize import word_tokenize\n",
      "\n",
      "stopwords = stopwords.words(\"english\")\n",
      "\n",
      "\"\"\"\n",
      "In this part we get features of a tweet by doing:\n",
      "1. removing stopwords\n",
      "2. remove digits\n",
      "3. remove character repitions\n",
      "\"\"\"\n",
      "\n",
      "def removeCharRepetition(word):\n",
      "    # regex pattern to remove repetitions\n",
      "    pattern = re.compile(r\"(.)\\1{1,}\", re.DOTALL)\n",
      "    return pattern.sub(r\"\\1\\1\", word)\n",
      "\n",
      "def getFeatureVector(tweet):\n",
      "    \n",
      "    # tokenize the tweet into corresponding words\n",
      "    word_list = word_tokenize(tweet)\n",
      "    \n",
      "    # remove storwords\n",
      "    processed_word_list_1 = [w for w in word_list if w not in stopwords]\n",
      "    \n",
      "    # remove character repitions like foooooottttballllllssss\n",
      "    processed_word_list_2 = [removeCharRepetition(w) for w in processed_word_list_1]\n",
      "    \n",
      "    # get only alhabetical words like happy but not 11:45am etc\n",
      "    processed_word_list_3 = [w for w in processed_word_list_2 if not(w.isdigit())]\n",
      "    \n",
      "    return processed_word_list_3"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 57
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "------\n",
      "### Helper functions completed\n",
      "\n",
      "From below the actual training stuff happens"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# here we process the training samples\n",
      "\n",
      "# create featuresList for all training tweets by creating a list of all the features\n",
      "# replace the actual tweets just by theor features and their sentiment\n",
      "\n",
      "# to get features of all training tweets in onego\n",
      "from nltk.classify.util import apply_features\n",
      "\n",
      "features_list = []\n",
      "tweets = []\n",
      "\n",
      "# this function creates a featurelist of the form\n",
      "\"\"\"\n",
      "{\n",
      "    'contains(bummer)': True,             #notice this because it is present in the particular tweet\n",
      "    'contains(happy)': False,       # rest fals ebeacuse not present in a particular tweet\n",
      "    'contains(hopeless)': False,\n",
      "    'contains(aww)': True,       #notice this\n",
      "    'contains(mutiny)': False,\n",
      "    .....\n",
      "}\n",
      "\"\"\"\n",
      "def extractFeatures(tweet):\n",
      "    global features_list\n",
      "    tweet_words = set(tweet)\n",
      "    features = {}\n",
      "    for word in features_list:\n",
      "        features['contains(%s)' % word] = (word in tweet_words)\n",
      "    return features\n",
      "\n",
      "def createTrainingSet(tweetsList,sentiment):\n",
      "    global features_list, tweets\n",
      "    for inptweet in tweetsList:\n",
      "        # get features of the tweet\n",
      "        features_vec = getFeatureVector(preProcessTweet(inptweet))\n",
      "        \n",
      "        # append the features to the massive features_list this is used for taining in nltk\n",
      "        features_list.extend(features_vec)\n",
      "\n",
      "        # replace the useless tweet with its features and sentiment\n",
      "        tweets.append((features_vec,sentiment))\n",
      "\n",
      "createTrainingSet(positive_training_tweets,\"positive\")\n",
      "createTrainingSet(negative_training_tweets,\"negative\")\n",
      "createTrainingSet(neutral_training_tweets,\"neutral\")\n",
      "    \n",
      "# remove duplicate and useless features\n",
      "# this can be done directly by removing the repitions or we can get the freqdistribution and then select the frequent features\n",
      "features_list = list(set(features_list))\n",
      "\n",
      "# for getting all the features in {contains('[]') : Bool} format we use below function\n",
      "training_set = apply_features(extractFeatures, tweets)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 59
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk import NaiveBayesClassifier\n",
      "import pickle\n",
      "\n",
      "# Train the classifier using the created training_set and dump it to a pickle, we can use this pickle for later use\n",
      "classifier = NaiveBayesClassifier.train(training_set)\n",
      "\n",
      "f = open('NaiveBayesClassifier.pickle', 'wb')\n",
      "pickle.dump(classifier, f)\n",
      "f.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 62
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pickle\n",
      "\n",
      "f = open('NaiveBayesClassifier.pickle')\n",
      "classifier = pickle.load(f)\n",
      "f.close()\n",
      "\n",
      "# basic test the classifier\n",
      "testTweet = 'Congrats @ravikiranj, i heard you wrote a new tech post on sentiment analysis'\n",
      "\n",
      "print classifier.classify(extractFeatures(getFeatureVector(preProcessTweet(testTweet))))\n",
      "\n",
      "\n",
      "\"\"\"\n",
      "for perforamnce testing we can take the remaining tweets in the training data set and check the accuracy of the model\n",
      "\"\"\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 63
    }
   ],
   "metadata": {}
  }
 ]
}