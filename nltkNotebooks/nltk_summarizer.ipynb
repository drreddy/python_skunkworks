{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook contains implementations for getting the summary of a block of text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation 1: Basic Implementation Based on Most Important Words\n",
    "---\n",
    "\n",
    "**This implementation uses frequency distribution of words to rank each sentence and select the top sentences**\n",
    "\n",
    "### Improvements (To Do)\n",
    "1. Based on pos tagging we can give preference to specific type of pos like nouns etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "import string\n",
    "\n",
    "class Summarizer:\n",
    "    stopwords = stopwords.words(\"english\")\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def get_useful_bag_of_words(self,text):\n",
    "        bag_of_words = [w for w in word_tokenize(text) if w not in (self.stopwords + list(string.punctuation))]\n",
    "        return bag_of_words\n",
    "    \n",
    "    def score_sent(self,sent,word_rank_list):\n",
    "        score = 0\n",
    "        for w in word_tokenize(sent.lower()):\n",
    "            if w in word_rank_list.keys():\n",
    "                score += word_rank_list[w]\n",
    "        return score\n",
    "    \n",
    "    def reorder_sents(self,sents,text):\n",
    "        sents.sort( lambda s1, s2: text.find(s1) - text.find(s2) )\n",
    "        return sents\n",
    "    \n",
    "    def get_summary(self,text,sent_num):\n",
    "        sane_text = unicode(text, \"utf-8\")\n",
    "        \n",
    "        word_freq_dist = FreqDist(self.get_useful_bag_of_words(sane_text.lower()))\n",
    "        \n",
    "        word_ranks = {}\n",
    "        for word in word_freq_dist:\n",
    "            word_ranks[word] = word_freq_dist.freq(word)\n",
    "            \n",
    "        sents = sent_tokenize(sane_text)\n",
    "        \n",
    "        if sent_num > len(sents):\n",
    "            print \"text already summarized !!!\"\n",
    "            return\n",
    "        \n",
    "        full_output = []\n",
    "        for sent in sents:\n",
    "            temp = {}\n",
    "            temp['sent'] = sent\n",
    "            temp['score'] = self.score_sent(sent,word_ranks)\n",
    "            full_output.append(temp)\n",
    "            \n",
    "        sorted_full_output = sorted(full_output, key=lambda k: k['score'], reverse=True)\n",
    "        top_output_sents = [s['sent'] for s in sorted_full_output[:sent_num]]\n",
    "        reordered_sents = self.reorder_sents(top_output_sents,sane_text)\n",
    "        return \"\".join(reordered_sents)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All emotions, and that one particularly, were abhorrent to his cold, precise but admirably balanced mind.He was, I take it, the most perfect reasoning and observing machine that the world has seen, but as a lover he would have placed himself in a false position.But for the trained reasoner to admit such intrusions into his own delicate and finely adjusted temperament was to introduce a distracting factor which might throw a doubt upon all his mental results.Grit in a sensitive instrument, or a crack in one of his own high-power lenses, would not be more disturbing than a strong emotion in a nature such as his.And yet there was but one woman to him, and that woman was the late Irene Adler, of dubious and questionable memory.\n"
     ]
    }
   ],
   "source": [
    "test_summarizer = Summarizer()\n",
    "\n",
    "test_text = \"\"\"To Sherlock Holmes she is always the woman. I have\n",
    "seldom heard him mention her under any other name. In his eyes she\n",
    "eclipses and predominates the whole of her sex. It was not that he\n",
    "felt any emotion akin to love for Irene Adler. All emotions, and that\n",
    "one particularly, were abhorrent to his cold, precise but admirably\n",
    "balanced mind. He was, I take it, the most perfect reasoning and\n",
    "observing machine that the world has seen, but as a lover he would\n",
    "have placed himself in a false position. He never spoke of the softer\n",
    "passions, save with a gibe and a sneer. They were admirable things for\n",
    "the observer-excellent for drawing the veil from menâ€™s motives and\n",
    "actions. But for the trained reasoner to admit such intrusions into\n",
    "his own delicate and finely adjusted temperament was to introduce a\n",
    "distracting factor which might throw a doubt upon all his mental\n",
    "results. Grit in a sensitive instrument, or a crack in one of his own\n",
    "high-power lenses, would not be more disturbing than a strong emotion\n",
    "in a nature such as his. And yet there was but one woman to him, and\n",
    "that woman was the late Irene Adler, of dubious and questionable\n",
    "memory.\n",
    "\"\"\"\n",
    "\n",
    "test_text = ' '.join(test_text.strip().split('\\n'))\n",
    "\n",
    "summary = test_summarizer.get_summary(test_text,5)\n",
    "\n",
    "print summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation 2: Using Text Rank Algorithm\n",
    "---\n",
    "\n",
    "**This implementation creates a fully connected weighted graph and then using the inbuilt pagerank functionality (to score the sentences) of the networkx module of python creates the summary of a text block**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import itertools\n",
    "import math\n",
    "import networkx\n",
    "from collections import Counter\n",
    "\n",
    "class TextRankSummarizer:\n",
    "    graph = networkx.Graph()\n",
    "    stopwords = stopwords.words(\"english\")\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def get_vector(self,text):\n",
    "        words = [w for w in word_tokenize(text.lower()) if w not in self.stopwords]\n",
    "        return dict(Counter(words))\n",
    "        \n",
    "    def calc_cosine_sim(self,vector1,vector2):\n",
    "        \n",
    "        intersection_set = set(vector1.keys()) & set(vector2.keys())\n",
    "        dot_product = sum([vector1[x] * vector2[x] for x in intersection_set])\n",
    "\n",
    "        vector1_mag = math.sqrt( sum([vector1[x]**2 for x in vector1.keys()]) )\n",
    "        vector2_mag = math.sqrt( sum([vector2[x]**2 for x in vector2.keys()]) )\n",
    "        \n",
    "        denominator = vector1_mag*vector2_mag\n",
    "\n",
    "        if not denominator:\n",
    "            return 0.0\n",
    "        else:\n",
    "            return float(dot_product) / denominator\n",
    "    \n",
    "    def add_to_graph(self,n):\n",
    "        self.graph.add_nodes_from(n)\n",
    "        return\n",
    "    \n",
    "    def reorder_sents(self,sents,text):\n",
    "        sents.sort( lambda s1, s2: text.find(s1) - text.find(s2) )\n",
    "        return sents\n",
    "    \n",
    "    def get_summary(self,text,num_sent):\n",
    "        sane_text = unicode(text, \"utf-8\")            \n",
    "        sents = sent_tokenize(sane_text)\n",
    "        \n",
    "        if num_sent > len(sents):\n",
    "            print \"text already summarized !!!\"\n",
    "            return\n",
    "        \n",
    "        self.add_to_graph(sents)\n",
    "        \n",
    "        sent_pairs = list(itertools.combinations(sents, 2))\n",
    "        \n",
    "        for pair in sent_pairs:\n",
    "            sent1 = pair[0]\n",
    "            sent2 = pair[1]\n",
    "            vec1 = self.get_vector(sent1)\n",
    "            vec2 = self.get_vector(sent2)\n",
    "            \n",
    "            cosine_sim = self.calc_cosine_sim(vec1, vec2)\n",
    "            self.graph.add_edge(sent1, sent2, weight=cosine_sim)\n",
    "            \n",
    "        output_sents_score = networkx.pagerank(self.graph, weight='weight')\n",
    "        \n",
    "        output_sorted = sorted(output_sents_score, key=output_sents_score.get, reverse=True)\n",
    "        \n",
    "        reordered_sents = self.reorder_sents(output_sorted[:num_sent],sane_text)\n",
    "        \n",
    "        return \"\".join(reordered_sents)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All emotions, and that one particularly, were abhorrent to his cold, precise but admirably balanced mind.He was, I take it, the most perfect reasoning and observing machine that the world has seen, but as a lover he would have placed himself in a false position.He never spoke of the softer passions, save with a gibe and a sneer.Grit in a sensitive instrument, or a crack in one of his own high-power lenses, would not be more disturbing than a strong emotion in a nature such as his.And yet there was but one woman to him, and that woman was the late Irene Adler, of dubious and questionable memory.\n"
     ]
    }
   ],
   "source": [
    "test_summarizer = TextRankSummarizer()\n",
    "\n",
    "test_text = \"\"\"To Sherlock Holmes she is always the woman. I have\n",
    "seldom heard him mention her under any other name. In his eyes she\n",
    "eclipses and predominates the whole of her sex. It was not that he\n",
    "felt any emotion akin to love for Irene Adler. All emotions, and that\n",
    "one particularly, were abhorrent to his cold, precise but admirably\n",
    "balanced mind. He was, I take it, the most perfect reasoning and\n",
    "observing machine that the world has seen, but as a lover he would\n",
    "have placed himself in a false position. He never spoke of the softer\n",
    "passions, save with a gibe and a sneer. They were admirable things for\n",
    "the observer-excellent for drawing the veil from menâ€™s motives and\n",
    "actions. But for the trained reasoner to admit such intrusions into\n",
    "his own delicate and finely adjusted temperament was to introduce a\n",
    "distracting factor which might throw a doubt upon all his mental\n",
    "results. Grit in a sensitive instrument, or a crack in one of his own\n",
    "high-power lenses, would not be more disturbing than a strong emotion\n",
    "in a nature such as his. And yet there was but one woman to him, and\n",
    "that woman was the late Irene Adler, of dubious and questionable\n",
    "memory.\n",
    "\"\"\"\n",
    "\n",
    "test_text = ' '.join(test_text.strip().split('\\n'))\n",
    "\n",
    "summary = test_summarizer.get_summary(test_text,5)\n",
    "\n",
    "print summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
